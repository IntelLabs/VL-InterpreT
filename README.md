<p align="center">
  <img src="assets/logo.png" alt="VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers"/>
</p>


VL-InterpreT provides interactive visualizations for interpreting the attentions and hidden representations in vision-language transformers. It is a task agnostic and integrated tool that:
- Tracks a variety of statistics in attention heads throughout all layers for both vision and language components
- Visualizes cross-modal and intra-modal attentions through easily readable heatmaps
- Plots the hidden representations of vision and language tokens as they pass through the transformer layers.

# Paper
Our demo was accepted to CVPR 2022: <a href="https://arxiv.org/abs/2203.17247" target="_blank">VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers</a>

# Screencast Video
This video provides an overview of VL-Interpret and demonstrates a few interesting examples.

<p align="center">
  <a href="https://youtu.be/4Rj15Hi_Pdo" target="_blank"><img src="assets/screencast.png" alt="Video Demo" width="560"/></a>
</p>

# Live Demo
A live demo of the app (same as in the screencast video) is available <a href="http://vlinterpret38-env-2.eba-bgxp4fxk.us-east-2.elasticbeanstalk.com/" target="_blank">here</a>.

Please watch <a href="https://youtu.be/4Rj15Hi_Pdo" target="_blank">the screencast video</a> to get a sense of how to navigate the app. This demo contains 100 examples from the <a href="https://visualcommonsense.com/" target="_blank">Visual Commonsense Reasoning task</a> and shows the attention and hidden representations from the <a href="https://arxiv.org/abs/2109.10504" target="_blank">KD-VLP model</a>.

# Setup and Usage
Code will be available soon!
